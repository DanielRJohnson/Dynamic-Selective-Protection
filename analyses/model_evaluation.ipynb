{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d821c0e0e134d4881c77ccf4a7f080c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(button_style='success', description='Run over all matrices', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cea99eaec9064b109490bef1f520cbca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if \"running_all\" not in globals():\n",
    "    from ipywidgets import widgets\n",
    "    matrices = [\"bcsstk18\", \"cbuckle\", \"ct20stif\", \"raefsky4\", \"vanbody\"]\n",
    "    b = widgets.Button(description=\"Run over all matrices\", button_style=\"success\")\n",
    "    output = widgets.Output()\n",
    "\n",
    "    display(b, output)\n",
    "\n",
    "    def run_over_all_matrices(button):\n",
    "        global running_all\n",
    "        global matrix\n",
    "        running_all = True\n",
    "        with output:\n",
    "            for matrix in matrices:\n",
    "                print(f\"Running {matrix}...\")\n",
    "                %run ./model_evaluation.ipynb # will output at this cell rather than later\n",
    "            print(\"Finished!\")\n",
    "    b.on_click(run_over_all_matrices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"running_all\" not in globals():\n",
    "    matrix = \"ct20stif\"  # manually set to run over one matrix\n",
    "matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from joblib import load\n",
    "from glob import glob\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, fbeta_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_sizes = {\n",
    "    \"bcsstk18\": 1000,\n",
    "    \"cbuckle\": 1000,\n",
    "    \"ct20stif\": 1000,\n",
    "    \"raefsky4\": 1000,\n",
    "    \"vanbody\": 100\n",
    "}\n",
    "\n",
    "df = pd.read_csv(f\"data/{matrix}_{test_data_sizes[matrix]}.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values(\"slowdown\", inplace=False, ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = df[[\"error_iter\", \"pos_2norm\"]].to_numpy()\n",
    "y_test = df[\"slowdown\"].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = 1 / 98\n",
    "1 + (1 / p)\n",
    "\n",
    "# 1 + (1 / x) = 99\n",
    "# (1 / x) = 98\n",
    "# x = 1 / 98"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [load(fn) for fn in glob(f\"./models/{matrix}/*.pkl\")]\n",
    "model_names = [model.steps[-1][1].__class__.__name__ for model in models]\n",
    "ps = list(np.arange(1 / 98, 1, 0.01))\n",
    "\n",
    "reports = pd.DataFrame(index=ps, columns=model_names)\n",
    "reports.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in ps:\n",
    "    for model, name in zip(models, model_names):\n",
    "        preds = model.predict(X_test)\n",
    "        y_pred_clas = preds > (1 + (1 / p))\n",
    "        y_true_clas = y_test > (1 + (1 / p))\n",
    "        reports.loc[p][name] = {\n",
    "            \"accuracy\": accuracy_score(y_true_clas, y_pred_clas),\n",
    "            \"f1_score\": f1_score(y_true_clas, y_pred_clas, zero_division=0),\n",
    "            \"precision\": precision_score(y_true_clas, y_pred_clas, zero_division=0),\n",
    "            \"recall\": recall_score(y_true_clas, y_pred_clas),\n",
    "            \"f1b_0.5_score\": fbeta_score(y_true_clas, y_pred_clas, beta=0.5, zero_division=0),\n",
    "            \"f1b_2_score\": fbeta_score(y_true_clas, y_pred_clas, beta=2, zero_division=0)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, series in reports.items():\n",
    "    accuracies = [item[\"accuracy\"] for item in series]\n",
    "    best_acc_p = ps[np.argmax(accuracies)]\n",
    "    f1_scores = [scores[\"f1_score\"] for scores in series]\n",
    "    best_f1_p = ps[np.argmax(f1_scores)]\n",
    "\n",
    "    precisions = [scores[\"precision\"] for scores in series]\n",
    "    best_precision_p = ps[np.argmax(precisions)]\n",
    "    recalls = [scores[\"recall\"] for scores in series]\n",
    "    best_recall_p = ps[np.argmax(recalls)]\n",
    "\n",
    "    f1b_05s = [scores[\"f1b_0.5_score\"] for scores in series]\n",
    "    best_f1b_05_p = ps[np.argmax(f1b_05s)]\n",
    "    f1b_2s = [scores[\"f1b_2_score\"] for scores in series]\n",
    "    best_f1b_2_p = ps[np.argmax(f1b_2s)]\n",
    "\n",
    "\n",
    "    plt.plot(ps, accuracies, label=f\"accuracy\", c=\"r\")\n",
    "    plt.axvline(x=best_acc_p, linestyle='--', c=\"r\")\n",
    "    plt.annotate(f\"{round(np.max(accuracies), 3)}\", xy=(best_acc_p + 0.01, 0.5), c=\"r\")\n",
    "\n",
    "    plt.plot(ps, f1_scores, label=f\"f1-score\", c=\"g\")\n",
    "    plt.axvline(x=best_f1_p, linestyle='--', c=\"g\")\n",
    "    plt.annotate(f\"{round(np.max(f1_scores), 3)}\", xy=(best_f1_p + 0.01, 0.55), c=\"g\")\n",
    "\n",
    "    plt.plot(ps, precisions, label=f\"precision\", c=\"b\")\n",
    "    plt.axvline(x=best_precision_p, linestyle='--', c=\"b\")\n",
    "    plt.annotate(f\"{round(np.max(precisions), 3)}\", xy=(best_precision_p + 0.01, 0.6), c=\"b\")\n",
    "\n",
    "    plt.plot(ps, recalls, label=f\"recall\", c=\"y\")\n",
    "    plt.axvline(x=best_recall_p, linestyle='--', c=\"y\")\n",
    "    plt.annotate(f\"{round(np.max(recalls), 3)}\", xy=(best_recall_p + 0.01, 0.65), c=\"y\")\n",
    "\n",
    "    plt.plot(ps, f1b_05s, label=f\"f1beta=0.5\", c=\"magenta\")\n",
    "    plt.axvline(x=best_f1b_05_p, linestyle='--', c=\"magenta\")\n",
    "    plt.annotate(f\"{round(np.max(f1b_05s), 3)}\", xy=(best_f1b_05_p + 0.01, 0.7), c=\"magenta\")\n",
    "    \n",
    "    plt.plot(ps, f1b_2s, label=f\"f1beta=2\", c=\"grey\")\n",
    "    plt.axvline(x=best_f1b_2_p, linestyle='--', c=\"grey\")\n",
    "    plt.annotate(f\"{round(np.max(f1b_2s), 3)}\", xy=(best_f1b_2_p + 0.01, 0.75), c=\"grey\")\n",
    "\n",
    "    plt.title(name)\n",
    "    plt.xlabel(\"$p$\")\n",
    "    plt.ylabel(\"Classification Scores\")\n",
    "    plt.xlim(1/98, 1)\n",
    "    plt.ylim(0, 1)\n",
    "    plt.legend()\n",
    "    os.makedirs(f\"./figures/{matrix}\", exist_ok=True)\n",
    "    plt.savefig(f\"./figures/{matrix}/{name}.png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_metrics = pd.DataFrame()  # columns=[\"model\", \"metric\", \"value\"]\n",
    "\n",
    "avg_metrics[\"accuracy\"] = reports.apply(lambda col: np.mean([scores[\"accuracy\"] for scores in col]))\n",
    "avg_metrics[\"f1_score\"] = reports.apply(lambda col: np.mean([scores[\"f1_score\"] for scores in col]))\n",
    "avg_metrics[\"precision\"] = reports.apply(lambda col: np.mean([scores[\"precision\"] for scores in col]))\n",
    "avg_metrics[\"recall\"] = reports.apply(lambda col: np.mean([scores[\"recall\"] for scores in col]))\n",
    "avg_metrics[\"f1b_0.5_score\"] = reports.apply(lambda col: np.mean([scores[\"f1b_0.5_score\"] for scores in col]))\n",
    "avg_metrics[\"f1b_2_score\"] = reports.apply(lambda col: np.mean([scores[\"f1b_2_score\"] for scores in col]))\n",
    "avg_metrics = avg_metrics.reset_index(names=[\"model\"])\n",
    "avg_metrics = avg_metrics.melt(id_vars=\"model\", var_name=\"metric\")\n",
    "\n",
    "avg_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_models = avg_metrics.loc[avg_metrics.groupby('metric')['value'].idxmax()]\n",
    "print(f'Best model for f1 is {best_models[\"model\"][best_models[\"metric\"] == \"f1_score\"].iloc[0]}')\n",
    "print(f'Best model for f1b_0.5 is {best_models[\"model\"][best_models[\"metric\"] == \"f1b_0.5_score\"].iloc[0]}')\n",
    "print(f'Best model for f1b_2 is {best_models[\"model\"][best_models[\"metric\"] == \"f1b_2_score\"].iloc[0]}')\n",
    "\n",
    "best_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.gcf().set_size_inches(12, 6)\n",
    "plt.gcf().set_dpi(100)\n",
    "sns.barplot(avg_metrics, x=\"model\", y=\"value\", hue=\"metric\", palette=\"hls\")\n",
    "plt.ylim(0, 1.3)\n",
    "plt.title(\"Avg Metrics by Model\")\n",
    "plt.legend(ncols=2)\n",
    "plt.savefig(f\"./figures/{matrix}/model_avg_metrics.png\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "main",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
