{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"running_all\" not in globals():\n",
    "    from ipywidgets import widgets\n",
    "    matrices = [\"ex10\", \"msc04515\", \"s1rmq4m1\", \"Na5\", \"bcsstk18\",\n",
    "                \"vibrobox\", \"cbuckle\", \"Pres_Poisson\", \"raefsky4\", \"vanbody\",\n",
    "                \"ct20stif\", \"cant\", \"bcircuit\", \"apache1\", \"consph\"]\n",
    "    b = widgets.Button(description=\"Run over all matrices\", button_style=\"success\")\n",
    "    output = widgets.Output()\n",
    "\n",
    "    display(b, output)\n",
    "\n",
    "    def run_over_all_matrices(button):\n",
    "        global running_all\n",
    "        global matrix\n",
    "        running_all = True\n",
    "        with output:\n",
    "            for matrix in matrices:\n",
    "                print(f\"Running {matrix}...\")\n",
    "                %run ./model_evaluation.ipynb # will output at this cell rather than later\n",
    "            print(\"Finished!\")\n",
    "    b.on_click(run_over_all_matrices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"running_all\" not in globals():\n",
    "    matrix = \"bcsstk18\"  # manually set to run over one matrix\n",
    "matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from joblib import load\n",
    "from json import dump\n",
    "from glob import glob\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, fbeta_score, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TESTING_DATA_SIZE = 1000\n",
    "\n",
    "df = pd.read_csv(f\"data/{matrix}_{TESTING_DATA_SIZE}.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_names = {\n",
    "    \"Ridge\": \"Polynomial Regression\",\n",
    "    \"RandomForestRegressor\": \"Random Forest\",\n",
    "    \"KNeighborsRegressor\": \"K-Nearest Neighbors\",\n",
    "    \"XGBRegressor\": \"XGBoost\",\n",
    "    \"LinearSVR\": \"Support Vector Machine\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = df[[\"error_iter\", \"pos_2norm\"]].to_numpy()\n",
    "y_test = df[\"slowdown\"].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = 1 / 98\n",
    "1 + (1 / p)\n",
    "\n",
    "# 1 + (1 / x) = 99\n",
    "# (1 / x) = 98\n",
    "# x = 1 / 98"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [load(fn) for fn in glob(f\"./models/{matrix}/*.pkl\")]\n",
    "model_names = [real_names[model.steps[-1][1].__class__.__name__] for model in models]\n",
    "ps = [1/98] + list(np.arange(0.02, 1.01, 0.01))\n",
    "\n",
    "reports = pd.DataFrame(index=ps, columns=model_names)\n",
    "reports.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparams = {name: {k: v for k, v in model.get_params().items() if \"__\" in k} \n",
    "               for name, model in zip(model_names, models)}\n",
    "with open(f\"./figures/{matrix}/hyperparams.json\", \"w\") as f:\n",
    "    dump(hyperparams, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in ps:\n",
    "    for model, name in zip(models, model_names):\n",
    "        preds = model.predict(X_test)\n",
    "        y_pred_clas = preds > (1 + (1 / p))\n",
    "        y_true_clas = y_test > (1 + (1 / p))\n",
    "        reports.loc[p][name] = {\n",
    "            \"Accuracy\": accuracy_score(y_true_clas, y_pred_clas),\n",
    "            \"$F_1$ Score\": f1_score(y_true_clas, y_pred_clas, zero_division=0),\n",
    "            \"Precision\": precision_score(y_true_clas, y_pred_clas, zero_division=0),\n",
    "            \"Recall\": recall_score(y_true_clas, y_pred_clas, zero_division=0),\n",
    "            r\"$F_\\beta$ Score ($\\beta = 0.5$)\": fbeta_score(y_true_clas, y_pred_clas, beta=0.5, zero_division=0),\n",
    "            r\"$F_\\beta$ Score ($\\beta = 2$)\": fbeta_score(y_true_clas, y_pred_clas, beta=2, zero_division=0)\n",
    "        }\n",
    "\n",
    "reports.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "palette = sns.color_palette(\"tab10\", 7)[1:]\n",
    "\n",
    "for name, series in reports.items():\n",
    "    accuracies = [item[\"Accuracy\"] for item in series]\n",
    "    best_acc_p = ps[np.argmax(accuracies)]\n",
    "    f1_scores = [scores[\"$F_1$ Score\"] for scores in series]\n",
    "    best_f1_p = ps[np.argmax(f1_scores)]\n",
    "\n",
    "    precisions = [scores[\"Precision\"] for scores in series]\n",
    "    best_precision_p = ps[np.argmax(precisions)]\n",
    "    recalls = [scores[\"Recall\"] for scores in series]\n",
    "    best_recall_p = ps[np.argmax(recalls)]\n",
    "\n",
    "    f1b_05s = [scores[r\"$F_\\beta$ Score ($\\beta = 0.5$)\"] for scores in series]\n",
    "    best_f1b_05_p = ps[np.argmax(f1b_05s)]\n",
    "    f1b_2s = [scores[r\"$F_\\beta$ Score ($\\beta = 2$)\"] for scores in series]\n",
    "    best_f1b_2_p = ps[np.argmax(f1b_2s)]\n",
    "\n",
    "    plt.gcf().set_size_inches(6, 4)\n",
    "    plt.gcf().set_dpi(100)\n",
    "\n",
    "    plt.plot(ps, accuracies, label=\"Accuracy\", c=palette[0])\n",
    "    # plt.axvline(x=best_acc_p, linestyle='--', c=\"r\")\n",
    "    # plt.annotate(f\"{round(np.max(accuracies), 3)}\", xy=(best_acc_p + 0.01, 0.5), c=\"r\")\n",
    "\n",
    "    plt.plot(ps, f1_scores, label=\"$F_1$ Score\", c=palette[1])\n",
    "    # plt.axvline(x=best_f1_p, linestyle='--', c=\"g\")\n",
    "    # plt.annotate(f\"{round(np.max(f1_scores), 3)}\", xy=(best_f1_p + 0.01, 0.55), c=\"g\")\n",
    "\n",
    "    plt.plot(ps, precisions, label=\"Precision\", c=palette[2])\n",
    "    # plt.axvline(x=best_precision_p, linestyle='--', c=\"b\")\n",
    "    # plt.annotate(f\"{round(np.max(precisions), 3)}\", xy=(best_precision_p + 0.01, 0.6), c=\"b\")\n",
    "\n",
    "    plt.plot(ps, recalls, label=\"Recall\", c=palette[3])\n",
    "    # plt.axvline(x=best_recall_p, linestyle='--', c=\"y\")\n",
    "    # plt.annotate(f\"{round(np.max(recalls), 3)}\", xy=(best_recall_p + 0.01, 0.65), c=\"y\")\n",
    "\n",
    "    plt.plot(ps, f1b_05s, label=rf\"$F_\\beta$ Score ($\\beta = 0.5$)\", c=palette[4])\n",
    "    # plt.axvline(x=best_f1b_05_p, linestyle='--', c=\"magenta\")\n",
    "    # plt.annotate(f\"{round(np.max(f1b_05s), 3)}\", xy=(best_f1b_05_p + 0.01, 0.7), c=\"magenta\")\n",
    "    \n",
    "    plt.plot(ps, f1b_2s, label=rf\"$F_\\beta$ Score ($\\beta = 2$)\", c=palette[5])\n",
    "    # plt.axvline(x=best_f1b_2_p, linestyle='--', c=\"grey\")\n",
    "    # plt.annotate(f\"{round(np.max(f1b_2s), 3)}\", xy=(best_f1b_2_p + 0.01, 0.75), c=\"grey\")\n",
    "\n",
    "    for i, metric in enumerate([accuracies, f1_scores, precisions, recalls, f1b_05s, f1b_2s]):\n",
    "        plt.axhline(y=sum(metric) / len(metric), linestyle='--', linewidth=0.75, c=palette[i])\n",
    "        \n",
    "    plt.title(f\"{matrix} - {name}\", weight=\"bold\")\n",
    "    plt.xlabel(\"$p$\")\n",
    "    plt.ylabel(\"Classification Scores\")\n",
    "    plt.xlim(1/98, 1)\n",
    "    plt.ylim(0, 1)\n",
    "    plt.legend(loc=(1.05, 0.25))\n",
    "    os.makedirs(f\"./figures/{matrix}\", exist_ok=True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"./figures/{matrix}/{name}_classification_metrics.png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_metrics = pd.DataFrame()  # columns=[\"model\", \"metric\", \"value\"]\n",
    "\n",
    "avg_metrics[\"Accuracy\"] = reports.apply(lambda col: np.mean([scores[\"Accuracy\"] for scores in col]))\n",
    "avg_metrics[\"$F_1$ Score\"] = reports.apply(\n",
    "    lambda col: np.mean([scores[\"$F_1$ Score\"] for scores in col]))\n",
    "avg_metrics[\"Precision\"] = reports.apply(lambda col: np.mean([scores[\"Precision\"] for scores in col]))\n",
    "avg_metrics[\"Recall\"] = reports.apply(lambda col: np.mean([scores[\"Recall\"] for scores in col]))\n",
    "avg_metrics[r\"$F_\\beta$ Score ($\\beta = 0.5$)\"] = reports.apply(\n",
    "    lambda col: np.mean([scores[r\"$F_\\beta$ Score ($\\beta = 0.5$)\"] for scores in col]))\n",
    "avg_metrics[r\"$F_\\beta$ Score ($\\beta = 2$)\"] = reports.apply(\n",
    "    lambda col: np.mean([scores[r\"$F_\\beta$ Score ($\\beta = 2$)\"] for scores in col]))\n",
    "avg_metrics = avg_metrics.reset_index(names=[\"model\"])\n",
    "avg_metrics = avg_metrics.melt(id_vars=\"model\", var_name=\"metric\")\n",
    "\n",
    "avg_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.gcf().set_size_inches(12, 6)\n",
    "plt.gcf().set_dpi(100)\n",
    "sns.barplot(avg_metrics, x=\"model\", y=\"value\", hue=\"metric\", palette=\"hls\")\n",
    "plt.ylim(0, 1)\n",
    "plt.xlabel(\"\")\n",
    "plt.ylabel(\"\")\n",
    "plt.title(f\"{matrix}\", weight=\"bold\")\n",
    "plt.legend(ncols=3, loc=\"lower center\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"./figures/{matrix}/avg_metrics_by_model.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "palette = sns.color_palette(\"tab10\", 6)[1:]\n",
    "\n",
    "for metric in [\"Accuracy\", \"Precision\", \"Recall\", \"$F_1$ Score\", r\"$F_\\beta$ Score ($\\beta = 0.5$)\",\n",
    "               r\"$F_\\beta$ Score ($\\beta = 2$)\"]:\n",
    "    metrics = pd.DataFrame()\n",
    "    for name, series in reports.items():\n",
    "        metrics[name] = [scores[metric] for scores in series]\n",
    "\n",
    "    plt.gcf().set_size_inches(4, 4)\n",
    "    plt.gcf().set_dpi(100)\n",
    "\n",
    "    sns.lineplot(metrics, dashes=False, palette=palette)\n",
    "    avgs = avg_metrics[avg_metrics[\"metric\"] == metric].drop(columns=\"metric\").reset_index()\n",
    "    for i, row in avgs.iterrows():\n",
    "        plt.axhline(y=row[\"value\"], linestyle='--', linewidth=0.75, c=palette[i])\n",
    "\n",
    "    plt.xticks([0, 20, 40, 60, 80, 100], labels=[0.0, 0.2, 0.4, 0.6, 0.8, 1.0])\n",
    "    plt.xlabel(\"$p$\")\n",
    "    plt.ylabel(metric)\n",
    "    plt.title(f\"{matrix}\", weight=\"bold\")\n",
    "    plt.tight_layout()\n",
    "    plt.legend(loc=(1.05, 0.25))\n",
    "    bs = \"\\\\\"\n",
    "    plt.savefig(f\"./figures/{matrix}/{metric.replace('$', '').replace(bs, '')}_by_model.png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "main",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
