{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"running_all\" not in globals():\n",
    "    from ipywidgets import widgets\n",
    "    matrices = [\"ex10\", \"msc04515\", \"s1rmq4m1\", \"Na5\", \"bcsstk18\",\n",
    "                \"vibrobox\", \"cbuckle\", \"Pres_Poisson\", \"raefsky4\", \"vanbody\",\n",
    "                \"ct20stif\", \"cant\", \"bcircuit\", \"apache1\", \"consph\"]\n",
    "    b = widgets.Button(description=\"Run over all matrices\", button_style=\"success\")\n",
    "    output = widgets.Output()\n",
    "\n",
    "    display(b, output)\n",
    "\n",
    "    def run_over_all_matrices(button):\n",
    "        global running_all\n",
    "        global matrix\n",
    "        running_all = True\n",
    "        with output:\n",
    "            for matrix in matrices:\n",
    "                print(f\"Running {matrix}...\")\n",
    "                %run ./overhead_analysis.ipynb # will output at this cell rather than later\n",
    "            print(\"Finished!\")\n",
    "    b.on_click(run_over_all_matrices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"running_all\" not in globals():\n",
    "    matrix = \"bcsstk18\"  # manually set to run over one matrix\n",
    "matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "from json import load\n",
    "sys.path.append(os.path.join(os.getcwd(), os.pardir))\n",
    "from io_utils import load_models, load_matrices_from_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ylims = {\n",
    "    \"apache1\": (0, 0.3),\n",
    "    \"bcircuit\": (0, 0.6),\n",
    "    \"bcsstk18\": (0.2, 1),\n",
    "    \"cant\": (0.8, 1),\n",
    "    \"cbuckle\": (0, 0.6),\n",
    "    \"consph\": (0, 0.02),\n",
    "    \"ct20stif\": (0, 1),\n",
    "    \"ex10\": (0.4, 0.9),\n",
    "    \"msc04515\": (0, 0.5),\n",
    "    \"Na5\": (0, 1),\n",
    "    \"Pres_Poisson\": (0, 0.6),\n",
    "    \"raefsky4\": (0, 0.2),\n",
    "    \"s1rmq4m1\": (0, 0.6),\n",
    "    \"vanbody\": (0.925, 1),\n",
    "    \"vibrobox\": (0.4, 0.65),\n",
    "}\n",
    "\n",
    "TESTING_DATA_SIZE = 1000\n",
    "\n",
    "df = pd.read_csv(f\"data/{matrix}_{TESTING_DATA_SIZE}.csv\")\n",
    "errorfree_iterations = df[\"errorfree_iterations\"][0]  # all the same\n",
    "n_rows = df[\"n_rows\"][0]  # all the same\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_models = {\n",
    "    \"apache1\": \"XGBRegressor\",\n",
    "    \"bcircuit\": \"XGBRegressor\",\n",
    "    \"bcsstk18\": \"XGBRegressor\",\n",
    "    \"cant\": \"KNeighborsRegressor\",\n",
    "    \"cbuckle\": \"KNeighborsRegressor\",\n",
    "    \"consph\": \"Ridge\",\n",
    "    \"ct20stif\": \"XGBRegressor\",\n",
    "    \"ex10\": \"RandomForestRegressor\",\n",
    "    \"msc04515\": \"RandomForestRegressor\",\n",
    "    \"Na5\": \"XGBRegressor\",\n",
    "    \"Pres_Poisson\": \"XGBRegressor\",\n",
    "    \"raefsky4\": \"RandomForestRegressor\",\n",
    "    \"s1rmq4m1\": \"RandomForestRegressor\",\n",
    "    \"vanbody\": \"RandomForestRegressor\",\n",
    "    \"vibrobox\": \"KNeighborsRegressor\",\n",
    "}\n",
    "\n",
    "model_path = f\"./models/{matrix}/best_{chosen_models[matrix]}.pkl\"\n",
    "model = load_models([model_path])[0]\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[[\"error_iter\", \"pos_2norm\"]].to_numpy()\n",
    "df[\"prot_score\"] = model.predict(X)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mats = load_matrices_from_dir(\"../matrices/raw\", subset=[matrix])\n",
    "mat = list(mats.values())[0]\n",
    "mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"../matrices/2norms/{matrix}_pos_2norms.json\") as f:\n",
    "    pos_2norms = load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_preds = pd.DataFrame([[i, pos_2norms[str(pos)], pos] for pos in range(mat.shape[0])\n",
    "                         for i in range(errorfree_iterations)], columns=[\"i\", \"2norm\", \"rowid\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_preds[\"output\"] = model.predict(df_preds[[\"i\", \"2norm\"]])\n",
    "df_preds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = [1/98] + list(np.arange(0.02, 1.01, 0.01))\n",
    "percentages = np.arange(0.01, 1.0, 0.01)\n",
    "nonerror_runs_by_p = {p: int((len(df) / p) - len(df)) for p in ps}\n",
    "max_nonerror_runs = int((len(df) / min(ps)) - len(df))\n",
    "solve_iterations = np.append(df[\"solve_iterations\"], [errorfree_iterations] * max_nonerror_runs)\n",
    "slowdowns = np.append(df[\"slowdown\"], [1] * max_nonerror_runs)\n",
    "errorfree_op_count = errorfree_iterations * n_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_overheads(error_iterations, n_protections):\n",
    "    return ((error_iterations * n_rows + n_protections) - errorfree_op_count) / errorfree_op_count\n",
    "\n",
    "def protect(error_iterations, protections):\n",
    "    return np.vectorize(lambda i: error_iterations[i] if not protections[i]\n",
    "                        else errorfree_iterations)(range(len(error_iterations)))\n",
    "\n",
    "def make_p_overhead_dataframe(ohs_by_p):\n",
    "    return pd.concat([pd.DataFrame({\"p\": [ps[i]] * len(os), \"overhead\": os})\n",
    "                         for i, os in enumerate(ohs_by_p)], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nonprot_overheads_by_p = []\n",
    "\n",
    "for p in ps:\n",
    "    # need to pad dataset to add non-error runs\n",
    "    n_nonerror_runs = nonerror_runs_by_p[p]\n",
    "    data_size = n_nonerror_runs + len(df)\n",
    "\n",
    "    nonprot_overheads = compute_overheads(solve_iterations[:data_size], 0)\n",
    "    nonprot_overheads_by_p.append(nonprot_overheads)\n",
    "\n",
    "nonprot_df = make_p_overhead_dataframe(nonprot_overheads_by_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prot_overheads_by_p = []\n",
    "\n",
    "for p in ps:\n",
    "    # need to pad dataset to add non-error runs\n",
    "    n_nonerror_runs = nonerror_runs_by_p[p]\n",
    "    data_size = n_nonerror_runs + len(df)\n",
    "    \n",
    "    # for the purpose of choosing solve_iterations or errorfree_iterations, did_protect will\n",
    "    # always be False for nonerror runs, but this is fine because n_protections is computed\n",
    "    # later for the purposes of calculating overhead\n",
    "    protections = np.append(df[\"prot_score\"] > (1 + (1 / p)), [False] * n_nonerror_runs)\n",
    "\n",
    "    prot_iterations = protect(solve_iterations[:data_size], protections)\n",
    "    n_protections = (df_preds[\"output\"] > (1 + (1 / p))).sum()\n",
    "\n",
    "    prot_overheads = compute_overheads(prot_iterations, n_protections)\n",
    "    prot_overheads_by_p.append(prot_overheads)\n",
    "\n",
    "prot_df = make_p_overhead_dataframe(prot_overheads_by_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_overheads = []\n",
    "for perc in percentages:\n",
    "    protections = np.random.rand(len(df)) < perc\n",
    "    prot_iterations = protect(solve_iterations[:len(df)], protections)\n",
    "    n_protections = int(errorfree_iterations * n_rows * perc)\n",
    "    prot_overheads = compute_overheads(prot_iterations, n_protections)\n",
    "    random_overheads.append(prot_overheads.mean())\n",
    "    \n",
    "best_random_percentage = percentages[np.argmin(random_overheads)]\n",
    "rand_overheads_by_p = []\n",
    "\n",
    "for p in ps:\n",
    "    # need to pad dataset to add non-error runs\n",
    "    n_nonerror_runs = nonerror_runs_by_p[p]\n",
    "    data_size = n_nonerror_runs + len(df)\n",
    "    \n",
    "    protections = np.random.rand(data_size) < best_random_percentage\n",
    "\n",
    "    prot_iterations = protect(solve_iterations[:data_size], protections)\n",
    "    n_protections = int(errorfree_iterations * n_rows * best_random_percentage)\n",
    "    \n",
    "    prot_overheads = compute_overheads(prot_iterations, n_protections)\n",
    "    rand_overheads_by_p.append(prot_overheads)\n",
    "\n",
    "rand_df = make_p_overhead_dataframe(rand_overheads_by_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2ns = np.array(list(pos_2norms.values()))\n",
    "r2n_overheads = []\n",
    "for perc in percentages:\n",
    "    cutoff = np.quantile(r2ns, 1 - perc)\n",
    "    protections = df[\"pos_2norm\"] >= cutoff\n",
    "    prot_iterations = protect(solve_iterations[:len(df)], protections)\n",
    "    n_protections = errorfree_iterations * (r2ns >= cutoff).sum()\n",
    "    prot_overheads = compute_overheads(prot_iterations, n_protections)\n",
    "    r2n_overheads.append(prot_overheads.mean())\n",
    "\n",
    "best_r2n_percentage = percentages[np.argmin(r2n_overheads)]\n",
    "cutoff = np.quantile(r2ns, 1 - best_r2n_percentage)\n",
    "r2n_overheads_by_p = []\n",
    "\n",
    "for p in ps:\n",
    "    # need to pad dataset to add non-error runs\n",
    "    n_nonerror_runs = nonerror_runs_by_p[p]\n",
    "    data_size = n_nonerror_runs + len(df)\n",
    "\n",
    "    protections = np.append(df[\"pos_2norm\"] >= cutoff, [False] * n_nonerror_runs)\n",
    "\n",
    "    prot_iterations = protect(solve_iterations[:data_size], protections)\n",
    "    n_protections = errorfree_iterations * (r2ns >= cutoff).sum()\n",
    "    \n",
    "    prot_overheads = compute_overheads(prot_iterations, n_protections)\n",
    "    r2n_overheads_by_p.append(prot_overheads)\n",
    "\n",
    "r2n_df = make_p_overhead_dataframe(r2n_overheads_by_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.gcf().set_size_inches(4, 4)\n",
    "plt.gcf().set_dpi(100)\n",
    "\n",
    "sns.lineplot(nonprot_df, x=\"p\", y=\"overhead\",\n",
    "             label=\"No Protection\", c=\"red\")\n",
    "sns.lineplot(r2n_df, x=\"p\", y=\"overhead\",\n",
    "             label=f\"Best 2-Norm %\", c=\"blue\")\n",
    "sns.lineplot(prot_df, x=\"p\", y=\"overhead\",\n",
    "         label=\"Our Scheme\", c=\"green\")\n",
    "# sns.lineplot(rand_df, x=\"p\", y=\"overhead\",\n",
    "#              label=f\"Best Random % ({best_random_percentage * 100}%)\", c=\"blue\")\n",
    "# plt.plot(ps, [1] * len(ps), label=\"Full Protection (1)\", c=\"black\")\n",
    "\n",
    "def formatter(x, pos):\n",
    "    del pos\n",
    "    return str(round(x * 100)) + \"%\"\n",
    "\n",
    "plt.gca().yaxis.set_major_formatter(formatter)\n",
    "\n",
    "plt.xlabel(\"$p$\")\n",
    "plt.ylabel(\"Mean Overhead\")\n",
    "plt.title(f\"{matrix}\", weight=\"bold\")\n",
    "plt.grid()\n",
    "plt.xlim(0.01, 1)\n",
    "plt.ylim(*ylims[matrix])\n",
    "plt.tight_layout()\n",
    "plt.legend(bbox_to_anchor=(1.05, 0.39), loc=(1.05, 0.25))\n",
    "plt.savefig(f\"./figures/{matrix}/mean_overheads.png\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_prots_per_p = []\n",
    "\n",
    "for p in ps:\n",
    "    n_protections = (df_preds[\"output\"] > (1 + (1 / p))).sum()\n",
    "    n_prots_per_p.append(n_protections)\n",
    "\n",
    "s = pd.Series(n_prots_per_p)\n",
    "plt.plot(ps, s)\n",
    "plt.title(\"n_protections for each $p$\")\n",
    "plt.xlabel(\"$p$\")\n",
    "plt.ylabel(\"n_protections\")\n",
    "plt.savefig(f\"./figures/{matrix}/n_protections_for_each_p.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prot_avgs = prot_df.groupby(\"p\").mean().rename(columns={\"overhead\": \"prot_overhead\"})\n",
    "nonprot_avgs = nonprot_df.groupby(\"p\").mean().rename(columns={\"overhead\": \"nonprot_overhead\"})\n",
    "rand_avgs = rand_df.groupby(\"p\").mean().rename(columns={\"overhead\": \"rand_overhead\"})\n",
    "r2n_avgs = r2n_df.groupby(\"p\").mean().rename(columns={\"overhead\": \"r2n_overhead\"})\n",
    "\n",
    "avgs = prot_avgs.merge(nonprot_avgs, on=\"p\").merge(rand_avgs, on=\"p\").merge(r2n_avgs, on=\"p\")\n",
    "avgs.to_csv(f\"./figures/{matrix}/mean_overheads.csv\")\n",
    "avgs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "main",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
